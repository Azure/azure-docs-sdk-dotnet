### YamlMime:NetMember
type: method
members:
- returnsWithMoniker:
    type:
    - value: <xref href="Azure.AI.ContentSafety.AnalyzeTextResult?alt=Azure.AI.ContentSafety.AnalyzeTextResult&text=AnalyzeTextResult" data-throw-if-not-resolved="True"/>
    description: A new <xref data-throw-if-not-resolved="true" uid="Azure.AI.ContentSafety.AnalyzeTextResult"></xref> instance for mocking.
  parameters:
  - namesWithMoniker:
    - value: blocklistsMatchResults
    description: The details of blocklist match.
    type: <xref href="System.Collections.Generic.IEnumerable`1?alt=System.Collections.Generic.IEnumerable&text=IEnumerable" data-throw-if-not-resolved="True"/>&lt;<xref href="Azure.AI.ContentSafety.TextBlocklistMatchResult?alt=Azure.AI.ContentSafety.TextBlocklistMatchResult&text=TextBlocklistMatchResult" data-throw-if-not-resolved="True"/>&gt;
  - namesWithMoniker:
    - value: hateResult
    description: Analysis result for Hate category.
    type: <xref href="Azure.AI.ContentSafety.TextAnalyzeSeverityResult?alt=Azure.AI.ContentSafety.TextAnalyzeSeverityResult&text=TextAnalyzeSeverityResult" data-throw-if-not-resolved="True"/>
  - namesWithMoniker:
    - value: selfHarmResult
    description: Analysis result for SelfHarm category.
    type: <xref href="Azure.AI.ContentSafety.TextAnalyzeSeverityResult?alt=Azure.AI.ContentSafety.TextAnalyzeSeverityResult&text=TextAnalyzeSeverityResult" data-throw-if-not-resolved="True"/>
  - namesWithMoniker:
    - value: sexualResult
    description: Analysis result for Sexual category.
    type: <xref href="Azure.AI.ContentSafety.TextAnalyzeSeverityResult?alt=Azure.AI.ContentSafety.TextAnalyzeSeverityResult&text=TextAnalyzeSeverityResult" data-throw-if-not-resolved="True"/>
  - namesWithMoniker:
    - value: violenceResult
    description: Analysis result for Violence category.
    type: <xref href="Azure.AI.ContentSafety.TextAnalyzeSeverityResult?alt=Azure.AI.ContentSafety.TextAnalyzeSeverityResult&text=TextAnalyzeSeverityResult" data-throw-if-not-resolved="True"/>
  type: method
  uid: Azure.AI.ContentSafety.ContentSafetyModelFactory.AnalyzeTextResult(System.Collections.Generic.IEnumerable{Azure.AI.ContentSafety.TextBlocklistMatchResult},Azure.AI.ContentSafety.TextAnalyzeSeverityResult,Azure.AI.ContentSafety.TextAnalyzeSeverityResult,Azure.AI.ContentSafety.TextAnalyzeSeverityResult,Azure.AI.ContentSafety.TextAnalyzeSeverityResult)
  commentId: M:Azure.AI.ContentSafety.ContentSafetyModelFactory.AnalyzeTextResult(System.Collections.Generic.IEnumerable{Azure.AI.ContentSafety.TextBlocklistMatchResult},Azure.AI.ContentSafety.TextAnalyzeSeverityResult,Azure.AI.ContentSafety.TextAnalyzeSeverityResult,Azure.AI.ContentSafety.TextAnalyzeSeverityResult,Azure.AI.ContentSafety.TextAnalyzeSeverityResult)
  name: >-
    AnalyzeTextResult(IEnumerable<TextBlocklistMatchResult>, TextAnalyzeSeverityResult,

     TextAnalyzeSeverityResult, TextAnalyzeSeverityResult, TextAnalyzeSeverityResult)
  fullName: Azure.AI.ContentSafety.ContentSafetyModelFactory.AnalyzeTextResult(IEnumerable<TextBlocklistMatchResult>, TextAnalyzeSeverityResult, TextAnalyzeSeverityResult, TextAnalyzeSeverityResult, TextAnalyzeSeverityResult)
  nameWithType: ContentSafetyModelFactory.AnalyzeTextResult(IEnumerable<TextBlocklistMatchResult>, TextAnalyzeSeverityResult, TextAnalyzeSeverityResult, TextAnalyzeSeverityResult, TextAnalyzeSeverityResult)
  syntaxWithMoniker:
  - lang: csharp
    values:
    - value: public static Azure.AI.ContentSafety.AnalyzeTextResult AnalyzeTextResult (System.Collections.Generic.IEnumerable<Azure.AI.ContentSafety.TextBlocklistMatchResult> blocklistsMatchResults = default, Azure.AI.ContentSafety.TextAnalyzeSeverityResult hateResult = default, Azure.AI.ContentSafety.TextAnalyzeSeverityResult selfHarmResult = default, Azure.AI.ContentSafety.TextAnalyzeSeverityResult sexualResult = default, Azure.AI.ContentSafety.TextAnalyzeSeverityResult violenceResult = default);
  - lang: fsharp
    values:
    - value: 'static member AnalyzeTextResult : seq<Azure.AI.ContentSafety.TextBlocklistMatchResult> * Azure.AI.ContentSafety.TextAnalyzeSeverityResult * Azure.AI.ContentSafety.TextAnalyzeSeverityResult * Azure.AI.ContentSafety.TextAnalyzeSeverityResult * Azure.AI.ContentSafety.TextAnalyzeSeverityResult -> Azure.AI.ContentSafety.AnalyzeTextResult'
  - lang: vb
    values:
    - value: Public Shared Function AnalyzeTextResult (Optional blocklistsMatchResults As IEnumerable(Of TextBlocklistMatchResult) = Nothing, Optional hateResult As TextAnalyzeSeverityResult = Nothing, Optional selfHarmResult As TextAnalyzeSeverityResult = Nothing, Optional sexualResult As TextAnalyzeSeverityResult = Nothing, Optional violenceResult As TextAnalyzeSeverityResult = Nothing) As AnalyzeTextResult
  monikers:
  - azure-dotnet-preview
  summary: Initializes a new instance of AnalyzeTextResult.
  metadata: {}
uid: Azure.AI.ContentSafety.ContentSafetyModelFactory.AnalyzeTextResult*
commentId: Overload:Azure.AI.ContentSafety.ContentSafetyModelFactory.AnalyzeTextResult
namespace: Azure.AI.ContentSafety
name: AnalyzeTextResult
fullName: Azure.AI.ContentSafety.ContentSafetyModelFactory.AnalyzeTextResult
nameWithType: ContentSafetyModelFactory.AnalyzeTextResult
assembliesWithMoniker:
- value: Azure.AI.ContentSafety.dll
packagesWithMoniker:
- value: Azure.AI.ContentSafety v1.0.0-beta.1
devLangs:
- csharp
- vb
- fsharp
monikers:
- azure-dotnet-preview
metadata:
  api_name:
  - Azure.AI.ContentSafety.ContentSafetyModelFactory.AnalyzeTextResult
  api_location:
  - Azure.AI.ContentSafety.dll
  topic_type:
  - apiref
  api_type:
  - Assembly
  f1_keywords:
  - Azure.AI.ContentSafety.ContentSafetyModelFactory.AnalyzeTextResult
  - Azure::AI::ContentSafety::ContentSafetyModelFactory::AnalyzeTextResult
  - AnalyzeTextResult
  - ContentSafetyModelFactory.AnalyzeTextResult
  - ContentSafetyModelFactory::AnalyzeTextResult
  helpviewer_keywords:
  - ContentSafetyModelFactory.AnalyzeTextResult method [.NET]
  - AnalyzeTextResult method [.NET], class ContentSafetyModelFactory
  - ContentSafetyModelFactory.AnalyzeTextResult(IEnumerable<TextBlocklistMatchResult>, TextAnalyzeSeverityResult, TextAnalyzeSeverityResult, TextAnalyzeSeverityResult, TextAnalyzeSeverityResult) method [.NET]
  - AnalyzeTextResult(IEnumerable<TextBlocklistMatchResult>, TextAnalyzeSeverityResult, TextAnalyzeSeverityResult, TextAnalyzeSeverityResult, TextAnalyzeSeverityResult) method [.NET], class ContentSafetyModelFactory
  monikers:
  - azure-dotnet-preview
